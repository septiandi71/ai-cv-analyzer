# AI CV Analyzer - Backend

Automated CV and Project Report evaluation system using AI/LLM and RAG (Retrieval-Augmented Generation).

---

## ðŸ“‹ Table of Contents

1. [Candidate Information](#-candidate-information)
2. [Repository Link](#-repository-link)
3. [Approach & Design](#-approach--design)
4. [Results & Reflection](#-results--reflection)
5. [API Screenshots](#-api-screenshots)
6. [Bonus Features](#-bonus-features)
7. [Quick Start Guide](#-quick-start-guide)

---

## ï¿½ï¿½ Candi---

### 7. Code Quality & Implementation Details

#### Project Structure

```
src/
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ upload/              # File upload module
â”‚   â”‚   â”œâ”€â”€ upload.controller.ts
â”‚   â”‚   â”œâ”€â”€ upload.service.ts
â”‚   â”‚   â””â”€â”€ dto/
â”‚   â”œâ”€â”€ evaluation/          # Evaluation module
â”‚   â”‚   â”œâ”€â”€ evaluation.controller.ts
â”‚   â”‚   â”œâ”€â”€ evaluation.service.ts
â”‚   â”‚   â”œâ”€â”€ evaluation.processor.ts  # Bull worker
â”‚   â”‚   â””â”€â”€ dto/
â”‚   â”œâ”€â”€ llm/                 # LLM integration
â”‚   â”‚   â”œâ”€â”€ llm.service.ts
â”‚   â”‚   â””â”€â”€ providers/
â”‚   â”œâ”€â”€ ragie/               # RAG service
â”‚   â”‚   â””â”€â”€ ragie.service.ts
â”‚   â””â”€â”€ prisma/              # Database
â”‚       â””â”€â”€ prisma.service.ts
â”œâ”€â”€ common/                  # Shared utilities
â”‚   â”œâ”€â”€ guards/              # Auth guards
â”‚   â”œâ”€â”€ interceptors/        # Logging, transform
â”‚   â””â”€â”€ filters/             # Error handling
â””â”€â”€ main.ts
```

#### Code Quality Practices

**1. Type Safety with TypeScript:**
```typescript
// DTOs with validation
export class CreateEvaluationDto {
  @IsString()
  @IsNotEmpty()
  jobTitle: string;

  @IsUUID()
  cvFileId: string;

  @IsUUID()
  projectReportFileId: string;
}

// Prisma-generated types
import { Prisma, EvaluationStatus } from '@prisma/client';
```

**2. Dependency Injection (NestJS):**
```typescript
@Injectable()
export class EvaluationService {
  constructor(
    private readonly prisma: PrismaService,
    private readonly llmService: LLMService,
    private readonly ragieService: RagieService,
    @InjectQueue('evaluation') private evaluationQueue: Queue,
  ) {}
}
```

**3. Error Handling Patterns:**
```typescript
// Custom exceptions
throw new BadRequestException('Invalid PDF format');
throw new NotFoundException(`Job ${jobId} not found`);

// Global exception filter
@Catch()
export class AllExceptionsFilter implements ExceptionFilter {
  catch(exception: unknown, host: ArgumentsHost) {
    const ctx = host.switchToHttp();
    const response = ctx.getResponse();
    const status = exception instanceof HttpException 
      ? exception.getStatus() 
      : 500;
    
    response.status(status).json({
      statusCode: status,
      timestamp: new Date().toISOString(),
      message: exception.message,
    });
  }
}
```

**4. Logging Strategy:**
```typescript
// Structured logging with context
this.logger.log(`Starting evaluation for job ${jobTitle}`, 'EvaluationProcessor');
this.logger.error(`LLM API failed: ${error.message}`, error.stack, 'LLMService');
this.logger.warn(`RAG retrieval returned no results, using fallback`);

// Performance logging
const startTime = Date.now();
// ... process
const duration = Date.now() - startTime;
this.logger.log(`Evaluation completed in ${duration}ms`);
```

**5. Modularity & Reusability:**
```typescript
// Shared LLM service used by multiple modules
export class LLMService {
  async generate(prompt: string, options: LLMOptions): Promise<string> {
    // Handles provider fallback, retries, error handling
    // Can be reused for CV eval, project eval, summary generation
  }
}

// RAG service with generic retrieve method
export class RagieService {
  async retrieve(query: string, filter: any): Promise<RAGContext> {
    // Works for any document type (job_desc, rubric, etc.)
  }
}
```

**6. Configuration Management:**
```typescript
// Environment variables with validation
import { ConfigModule, ConfigService } from '@nestjs/config';

@Module({
  imports: [
    ConfigModule.forRoot({
      isGlobal: true,
      validationSchema: Joi.object({
        DATABASE_URL: Joi.string().required(),
        GEMINI_API_KEY: Joi.string().required(),
        REDIS_HOST: Joi.string().default('localhost'),
      }),
    }),
  ],
})
```

#### Testing Approach

**Manual Testing via Postman:**
- Complete collection with 10+ test cases
- Pre-request scripts for dynamic data
- Assertions for response validation
- Environment variables for different configs

**Test Coverage:**
```
âœ… Upload valid PDF files
âœ… Upload invalid file types
âœ… Upload oversized files
âœ… Evaluate with valid file IDs
âœ… Evaluate with non-existent file IDs
âœ… Poll result before completion
âœ… Poll result after completion
âœ… Health check endpoint
âœ… Authentication with valid API key
âœ… Authentication with invalid API key
```

**Why No Unit Tests?**
- Time constraints (5-7 day project)
- Focused on core functionality first
- Postman collection serves as integration tests
- Future improvement: Add Jest unit tests for services

---

#### 6. Edge Cases Considered

#### 1. Malformed PDFs
**Scenario:** Corrupted or unreadable PDF files Information

**Full Name:** Devin Septiandi Gunawan 
**Email Address:** Septiandi71@gmail.com

---

## ðŸ”— Repository Link

**GitHub Repository:** https://github.com/septiandi71/ai-cv-analyzer

---

## ðŸŽ¯ Approach & Design

### 1. Initial Plan

#### Requirements Breakdown
When I first analyzed the requirements, I identified 4 core functional areas:

1. **File Upload & Processing** - Handle CV and Project Report PDFs
2. **Asynchronous Evaluation** - Long-running LLM calls need queue system
3. **LLM Integration** - Structured scoring with multiple criteria
4. **RAG Integration** - Ground truth context from job descriptions and rubrics

#### Key Assumptions & Scope
- PDFs are the only supported format (most common for professional documents)
- Evaluation takes 30-60 seconds â†’ requires job queue pattern
- Scoring must be consistent and reproducible
- System must handle LLM API failures gracefully

---

### 2. System & Database Design

#### System Architecture Diagram

![System Architecture](https://i.imgur.com/xaYgJyo.png)

**Architecture Overview:**

The system follows a **queue-based asynchronous pattern** with 6 main layers:

1. **ðŸ–¥ï¸ Client Layer** - Applications that consume the API (Postman, web frontends)
2. **ðŸŒ API Gateway** - NestJS REST API with API key authentication
3. **âš™ï¸ Service Layer** - Business logic services (Upload, Evaluation, PDF parsing)
4. **ðŸ”„ Queue Layer** - Bull + Redis for asynchronous job processing
5. **ðŸ¤– AI/ML Layer** - Google Gemini (LLM) and Ragie (RAG) external services
6. **ðŸ’¾ Data Layer** - PostgreSQL database via Prisma ORM + file storage

**Request Flow:**
1. **Upload Phase:** Client uploads PDF files â†’ Files stored with extracted text
2. **Evaluation Phase:** Client triggers evaluation â†’ Job queued immediately (non-blocking)
3. **Processing Phase:** Worker picks job â†’ Fetches RAG context â†’ Calls LLM â†’ Saves results
4. **Result Phase:** Client polls result endpoint â†’ Returns data when job completes

---

#### API Endpoints Design

```
POST   /upload              - Upload CV + Project Report (returns file IDs)
POST   /evaluate            - Start evaluation job (returns job ID)
GET    /result/:jobId       - Get evaluation results (polling endpoint)
GET    /health              - Health check (public, no auth)
```

**Authentication:** Simple API Key via `x-api-key` header for production security.

---

#### Database Schema

![Database Schema](https://i.imgur.com/dzKeo50.png)

**Entity Relationship Overview:**

- **`uploaded_files`** stores CV and Project Report PDFs with extracted text
- **`evaluation_jobs`** contains evaluation results and metadata
- **Relation:** One-to-many (one file can be used in multiple evaluation jobs)
- **Foreign Keys:** `cv_file_id` and `project_report_file_id` link to `uploaded_files`

**Design Rationale:**
- **extracted_text** - Stores parsed PDF text to avoid re-parsing (performance optimization)
- **Relations** - One-to-many between uploaded_files â†’ evaluation_jobs
- **JSON fields** - Flexible storage for detailed scoring breakdown
- **Error tracking** - Stores error messages and retry attempts for debugging
- **Timestamps** - Track job lifecycle: created â†’ started â†’ completed
- **Indexes** - Optimized queries on status, timestamps, and foreign keys
- **Cascade delete** - When file is deleted, related jobs are also removed

#### Job Queue / Long-Running Task Handling

**Technology:** Bull + Redis

```typescript
// Queue Configuration
{
  concurrency: 2,           // Process 2 evaluations in parallel
  attempts: 3,              // Retry failed jobs 3 times
  backoff: {
    type: 'exponential',
    delay: 5000             // 5s, 10s, 20s retry delays
  }
}
```

**Flow:**
1. Client calls `/evaluate` â†’ Job added to queue â†’ Returns `job_id` immediately
2. Worker picks up job â†’ Calls LLM APIs â†’ Updates database
3. Client polls `/result/:jobId` â†’ Returns status + results when complete

**Why this approach:**
- Non-blocking: API responds instantly
- Resilient: Automatic retries on failure
- Scalable: Can add more workers if needed

---

### 3. LLM Integration

#### Provider Selection

**Primary:** Google Gemini 2.5 Flash (via Direct API)  
**Backup:** Google Gemini 2.0 Flash

**Why Gemini Direct instead of OpenRouter?**

Initially, I tested Gemini via OpenRouter for fallback diversity. However, I discovered:
- **Same model, different results**: Gemini Flash via Direct API vs OpenRouter produced inconsistent scores (variance up to 0.5 points)
- **Root cause**: OpenRouter merges system + user prompts differently than Gemini's native format
- **Solution**: Use dual Gemini models from same API for consistent tokenization

**Comparison with Equivalent Models:**

| Model | Cost (per 1M tokens) | Speed | Context Window | Structured Output | Choice Reason |
|-------|---------------------|-------|----------------|-------------------|---------------|
| **Gemini 2.5 Flash** âœ… | $0.075 input / $0.30 output | ~2-3s | 1M tokens | âœ… Native JSON mode | **Best balance of cost, speed, and quality** |
| Gemini 2.0 Flash | $0.075 input / $0.30 output | ~2-3s | 1M tokens | âœ… Native JSON mode | Backup for rate limits |
| GPT-4o-mini | $0.15 input / $0.60 output | ~3-4s | 128k tokens | âœ… JSON mode | 2x more expensive, smaller context |
| GPT-4o | $2.50 input / $10 output | ~5-8s | 128k tokens | âœ… JSON mode | 30x more expensive, overkill for task |

#### Temperature Settings Evolution

| Attempt | Temperature | Result | Issue |
|---------|-------------|--------|-------|
| 1 | 0.7 (default) | Inconsistent scores | High variance across runs |
| 2 | 0.1 | Still some variance | Acceptable but not perfect |
| 3 | **0.0** (final) | **Perfectly consistent** | âœ… Works after prompt simplification |

**Final Configuration:**
```typescript
// CV & Project Evaluation
{ temperature: 0.0, maxTokens: 4096 }

// Summary Generation (needs creativity)
{ temperature: 0.8, maxTokens: 2048 }
```

#### Prompt Design Strategy

**Philosophy:** Simple, clear prompts with RAG context > Complex rubrics

**Key Design Decisions:**
1. **RAG-first:** Use ground truth job descriptions when available
2. **Structured output:** Force JSON-only responses (no markdown code blocks)
3. **Character limits:** Prevent overly verbose feedback
4. **Consistent scoring:** Use same prompt structure every time

---

#### Actual Prompt Examples (Complete Implementation)

**1. CV Evaluation Prompt (with RAG context):**

```typescript
// System Prompt
const systemPrompt = `You are an expert technical recruiter evaluating a candidate's CV for a Backend Developer position.

=== JOB REQUIREMENTS (Ground Truth from RAG) ===
Required Skills:
- 3+ years experience with Node.js and TypeScript
- Strong understanding of RESTful API design
- Experience with PostgreSQL and database optimization
- Familiarity with Docker and cloud deployment (AWS/GCP)
- Strong problem-solving and debugging skills

Preferred Qualifications:
- Experience with NestJS or similar frameworks
- Knowledge of microservices architecture
- CI/CD pipeline experience
- Open source contributions

=== EVALUATION CRITERIA ===
Score each criterion from 1-5:
1. Technical Skills Match (40% weight)
2. Experience Level (25% weight)
3. Relevant Achievements (20% weight)
4. Cultural Fit & Soft Skills (15% weight)

IMPORTANT:
- Respond with ONLY valid JSON, no markdown code blocks
- Keep feedback concise (max 100 characters per criterion)
- Be objective and evidence-based
- Match rate = weighted average of all scores`

// User Prompt
const userPrompt = `Evaluate this CV:

${cvText}

Respond in this exact JSON format:
{
  "matchRate": 3.75,
  "technicalSkills": {
    "score": 4,
    "feedback": "Strong Node.js & TypeScript. Lacks Docker experience."
  },
  "experience": {
    "score": 4,
    "feedback": "5 years backend dev. Relevant project experience."
  },
  "achievements": {
    "score": 3,
    "feedback": "Led team of 3. No measurable impact metrics."
  },
  "culturalFit": {
    "score": 4,
    "feedback": "Collaborative, mentions pair programming."
  }
}`
```

**2. Project Evaluation Prompt (with RAG context):**

```typescript
// System Prompt
const systemPrompt = `You are a senior software engineer evaluating a technical project report.

=== PROJECT REQUIREMENTS (Ground Truth from RAG) ===
Expected Deliverables:
- RESTful API with CRUD operations
- Database integration with proper schema design
- Error handling and input validation
- API documentation (Swagger/OpenAPI)
- Clean, modular code structure
- README with setup instructions

Scoring Criteria:
1. Correctness (30%): Does it meet requirements?
2. Code Quality (25%): Clean, maintainable, follows best practices?
3. Architecture (20%): Good design decisions, modularity?
4. Documentation (15%): Clear README, API docs, comments?
5. Error Handling (10%): Graceful failures, validation?

IMPORTANT:
- Respond with ONLY valid JSON
- Feedback must be specific and actionable
- Score conservatively (3/5 = meets expectations)`

// User Prompt  
const userPrompt = `Evaluate this project report:

${projectText}

Respond in this exact JSON format:
{
  "score": 3.2,
  "correctness": {
    "score": 3,
    "feedback": "All CRUD endpoints present. Missing input validation."
  },
  "codeQuality": {
    "score": 4,
    "feedback": "Clean structure. Good use of TypeScript types."
  },
  "architecture": {
    "score": 3,
    "feedback": "Modular services. Could use better error middleware."
  },
  "documentation": {
    "score": 3,
    "feedback": "Basic README. Swagger docs present but incomplete."
  },
  "errorHandling": {
    "score": 3,
    "feedback": "Try-catch blocks present. Lacks global error handler."
  }
}`
```

**3. Summary Generation Prompt:**

```typescript
// System Prompt (Higher temperature for natural language)
const systemPrompt = `You are an experienced technical recruiter writing a brief candidate summary.

Write a 2-3 sentence summary that:
- Highlights key strengths from CV and project
- Mentions any concerns or gaps
- Provides hiring recommendation
- Uses natural, professional language
- Does NOT mention explicit scores or numbers

IMPORTANT: No bold text, no bullet points, just clean prose.`

// User Prompt
const userPrompt = `Based on these evaluations:

CV Evaluation:
- Match Rate: ${cvResult.matchRate}/5
- Technical Skills: ${cvResult.technicalSkills.score}/5 - ${cvResult.technicalSkills.feedback}
- Experience: ${cvResult.experience.score}/5 - ${cvResult.experience.feedback}

Project Evaluation:
- Overall Score: ${projectResult.score}/5
- Code Quality: ${projectResult.codeQuality.score}/5 - ${projectResult.codeQuality.feedback}
- Architecture: ${projectResult.architecture.score}/5 - ${projectResult.architecture.feedback}

Write a concise professional summary for the hiring manager.`
```

**Example Output:**
```
"The candidate demonstrates strong technical skills in Node.js and TypeScript with 
relevant backend development experience. Their project showcases clean code structure 
and good architectural decisions, though documentation could be more comprehensive. 
Recommended for technical interview round."
```

#### Chaining Logic

**Parallel Execution for Performance:**

```typescript
// BEFORE: Sequential (slow)
const cvResult = await evaluateCV();        // ~18s
const projectResult = await evaluateProject(); // ~18s
// Total: ~36s

// AFTER: Parallel (fast)
const [cvResult, projectResult] = await Promise.all([
  evaluateCV(),
  evaluateProject()
]);
// Total: ~18s (50% faster!)
```

**Summary Generation Chain:**

```typescript
// Step 1: CV + Project evaluated independently
const cvEvaluation = { matchRate: 0.85, scores: {...} }
const projectEvaluation = { score: 3.2, scores: {...} }

// Step 2: Summary generated from both evaluations
const summary = await generateSummary(cvEvaluation, projectEvaluation)
// Uses higher temperature (0.8) for more natural language
```

---

### 4. RAG (Retrieval-Augmented Generation) Strategy

#### Technology Stack

**RAG Provider:** Ragie (RAG-as-a-Service)  
**Why Ragie?**
- No need to manage vector database infrastructure
- Built-in chunking and embedding strategies
- Simple API for retrieval

#### Document Types Indexed

```typescript
1. Job Descriptions (type: 'job_description')
   - Backend Developer requirements
   - Technical qualifications
   
2. Case Study Brief (type: 'case_study_brief')
   - Project requirements
   - Expected deliverables
   
3. Scoring Rubrics (type: 'scoring_rubric')
   - CV evaluation criteria
   - Project scoring guidelines
```

#### Retrieval Strategy

```typescript
// Semantic search configuration
{
  topK: 5,                    // Retrieve top 5 most relevant chunks
  minScore: 0.1,              // Filter out irrelevant results
  filter: { type: '...' }     // Document type filtering
}

// Example: CV Evaluation
const jobRequirements = await ragie.retrieve({
  query: `${jobTitle} technical requirements`,
  filter: { type: 'job_description' }
})

const cvCriteria = await ragie.retrieve({
  query: 'CV evaluation criteria scoring rubric',
  filter: { type: 'scoring_rubric' }
})
```

#### Context Injection

RAG context is injected directly into LLM prompts:

```typescript
systemPrompt += `
=== JOB REQUIREMENTS (Ground Truth) ===
${jobRequirements.context}

=== EVALUATION CRITERIA (Ground Truth) ===
${cvCriteria.context}
`
```

**Fallback Behavior:**
If RAG retrieval fails (API error, no results), system uses hard-coded criteria to ensure evaluation continues.

---

### 5. Resilience & Error Handling

#### LLM API Failure Handling

**Multi-Layer Fallback Strategy:**

```typescript
// Layer 1: Primary provider with retries
try {
  return await callGeminiPrimary(prompt, { 
    retries: 3,
    backoff: 'exponential' 
  })
} catch (error) {
  // Layer 2: Backup provider
  try {
    return await callGeminiBackup(prompt)
  } catch (backupError) {
    // Layer 3: Job queue retry (up to 3 attempts)
    throw error // Bull will retry the entire job
  }
}
```

**Exponential Backoff:**
- Attempt 1: Immediate
- Attempt 2: Wait 5 seconds
- Attempt 3: Wait 10 seconds

**Rate Limit Detection:**
```typescript
if (error.status === 429) {
  this.logger.warn('Rate limited, trying backup provider...')
  // Immediately switch to backup, don't waste retry attempts
}
```

#### Timeout Handling

```typescript
// HTTP timeout for LLM calls
const response = await fetch(url, { 
  timeout: 60000  // 60s max per LLM call
})

// Job timeout
const job = await queue.add(data, {
  timeout: 120000  // 2 minutes max per evaluation
})
```

#### Randomness & Consistency

**Problem:** LLM responses are inherently random, even at low temperatures.

**Solutions Implemented:**
1. **Temperature 0.0** - Maximum determinism (no random sampling)
2. **Structured output** - Force JSON format to reduce variance
3. **Simple prompts** - Less room for interpretation
4. **Consistency instructions** - Explicit directive to score conservatively

**Result:** Achieved perfect consistency for CV evaluation (4.25/5.0 across multiple runs) and 0.0 variance for Project evaluation after prompt simplification.

---

### 6. Edge Cases Considered

#### 1. Malformed PDFs
**Scenario:** Corrupted or unreadable PDF files  
**Handling:**
```typescript
try {
  const text = await parsePDF(file)
  if (!text || text.length < 100) {
    throw new Error('PDF appears empty or corrupted')
  }
} catch (error) {
  return { 
    statusCode: 400, 
    message: 'Unable to extract text from PDF' 
  }
}
```

#### 2. Non-PDF Files
**Scenario:** User uploads .doc, .txt, or images  
**Handling:**
```typescript
const allowedMimeTypes = ['application/pdf']
if (!allowedMimeTypes.includes(file.mimetype)) {
  throw new BadRequestException('Only PDF files are supported')
}
```

#### 3. Extremely Large Files
**Scenario:** 100MB+ PDF files could crash the server  
**Handling:**
```typescript
// Multer configuration
{
  limits: { 
    fileSize: 10 * 1024 * 1024  // 10MB max
  }
}
```

#### 4. LLM Returns Non-JSON
**Scenario:** Despite instructions, LLM returns markdown or plain text  
**Handling:**
```typescript
function parseJSONResponse(content: string) {
  // Remove markdown code blocks
  content = content.replace(/```json\s*/gi, '').replace(/```/g, '')
  
  // Extract JSON object with regex
  const jsonMatch = content.match(/\{[\s\S]*\}/)
  if (!jsonMatch) {
    throw new Error('No JSON found in response')
  }
  
  return JSON.parse(jsonMatch[0])
}
```

#### 5. RAG Context Too Large
**Scenario:** Retrieved context exceeds token limits  
**Handling:**
```typescript
// Truncate context to 2000 chars to fit within prompt budget
systemPrompt += `\n\n${ragContext.substring(0, 2000)}\n`
```

---

## ðŸ“Š Results & Reflection

### What Worked Well

#### 1. âœ… RAG Integration
- Successfully indexed 4 document types (job descriptions, rubrics, case study)
- Retrieval is fast (~1s for 4 parallel queries)
- Context injection improved evaluation quality significantly
- Fallback to hard-coded criteria works seamlessly

#### 2. âœ… Parallel Execution
- Reduced evaluation time by 50% (36s â†’ 18s)
- No race conditions or data corruption
- Clean implementation with `Promise.all()`

#### 3. âœ… LLM Fallback Strategy
- Dual Gemini model approach provides reliability
- Never experienced complete API failure during testing
- Exponential backoff prevents API hammering

#### 4. âœ… Consistency Improvements
- Achieved perfect CV score consistency (0.0 variance)
- Project score variance reduced to 0.0 after prompt simplification
- Temperature 0.0 + simple prompts was the winning combination

#### 5. âœ… Clean Architecture
- Modular services (Upload, Evaluation, LLM, RAG, Prisma)
- Clear separation of concerns
- Easy to test individual components

---

### What Didn't Work as Expected

#### 1. âŒ Explicit Scoring Rubrics
**Problem:** Adding detailed scoring criteria INCREASED variance instead of reducing it.

**Why:** More detailed instructions = more interpretation paths = less consistency.

**Solution:** Reverted to simple prompts. Paradoxically, simpler = more consistent.

#### 2. âŒ OpenRouter for Gemini
**Problem:** Same Gemini model via OpenRouter gave different results than Direct API.

**Why:** OpenRouter merges system/user prompts differently, affecting tokenization.

**Solution:** Switched to dual Gemini models from same API (primary + backup).

#### 3. âŒ Model Version Inconsistency (2.5 Flash vs 2.0 Flash)
**Problem:** Even with identical prompts and files at temperature 0.0, Gemini 2.5 Flash (primary) and Gemini 2.0 Flash (backup) produce different evaluation results.

**Example:**
```
Same CV + Same Prompt + Temperature 0.0
- Gemini 2.5 Flash: matchRate = 4.25
- Gemini 2.0 Flash: matchRate = 3.75
```

---

### Evaluation of Results

#### Scoring Consistency

**CV Evaluation (Temperature 0.0):**
```json
Run 1: { matchRate: 4.25, technical_skills: 4, experience: 5 }
Run 2: { matchRate: 4.25, technical_skills: 4, experience: 5 }
Run 3: { matchRate: 4.25, technical_skills: 4, experience: 5 }
```
âœ… **Perfect consistency** - Scores are identical across runs.

**Project Evaluation (Temperature 0.0):**
```json
Run 1: { score: 2.3, correctness: 2, code_quality: 2 }
Run 2: { score: 2.3, correctness: 2, code_quality: 2 }
Run 3: { score: 2.3, correctness: 2, code_quality: 2 }
```
âœ… **Perfect consistency** after prompt simplification.

#### Why Results Are Good

1. **RAG Context:** Ground truth from job descriptions ensures relevant evaluation
2. **Temperature 0.0:** Eliminates randomness in scoring
3. **Simple Prompts:** Clear instructions reduce interpretation variance
4. **Structured Output:** JSON format forces consistency

---

### Future Improvements

#### With More Time

**1. Prompt Engineering & Quality Improvements**
```typescript
// A. Few-shot examples for better consistency
systemPrompt += `
Example evaluations:
CV with 5 years React â†’ technical_skills: 4/5
CV with 2 years React â†’ technical_skills: 3/5
`

// B. Chain-of-thought reasoning for transparency
userPrompt += `
Think step-by-step:
1. List key requirements from job description
2. Match each requirement to CV evidence
3. Assign scores with justification
`

// C. Self-consistency ensemble (run 3 times, take majority vote)
const results = await Promise.all([
  evaluateCV(prompt, temp=0.7),
  evaluateCV(prompt, temp=0.7),
  evaluateCV(prompt, temp=0.7)
])
const finalScore = getMajorityVote(results)
```
**Impact:** 
- Higher quality and more explainable scoring
- Reduce edge case failures
- Better alignment with human evaluators

**2. Model Version Lock & Score Normalization**
```typescript
// Normalize scores across model versions
const scoreAdjustment = {
  'gemini-2.5-flash': 1.0,    // baseline
  'gemini-2.0-flash': 1.15    // tends to score 15% lower
}

const normalizedScore = rawScore * scoreAdjustment[modelVersion]
```
**Impact:** Consistent scores even during failover between primary/backup models.

**3. Multi-language Support**
```typescript
// Detect CV language and adjust prompts
const detectedLang = detectLanguage(cvText)
const localizedPrompt = getPromptTemplate(detectedLang)
```
**Impact:** Support international candidates.

#### Constraints That Affected Solution

**1. Time Constraints**
- **Allocated:** 5-7 days
- **Impact:** Focused on core functionality over polish

**2. API Rate Limits**
- **Gemini Free Tier:** 15 RPM (requests per minute)
- **Mitigation:** Queue system ensures we stay under limits

**3. LLM Token Limits**
- **Gemini Flash:** 32k context window
- **Impact:** Had to truncate RAG context to 2000 chars

---

## ðŸ“¸ API Screenshots

### 1. Upload Files

### 2. Start Evaluation

### 3. Get Result (COMPLETED)

---

## ðŸŽ Bonus Features (20+ Additional Implementations)

### Core Enhancements

1. **ðŸ” API Key Authentication**
   - Custom guard checks `x-api-key` header
   - Environment-based key management
   - Protects all endpoints except `/health`

2. **ðŸ”„ Dual LLM Provider Strategy**
   - Primary: Gemini 2.5 Flash (latest model)
   - Backup: Gemini 2.0 Flash (fallback)
   - Automatic failover on errors
   - Tracks which model was used in database

3. **ðŸ“Š RAG Fallback System**
   - Tries Ragie API first for ground truth
   - Falls back to hard-coded criteria if unavailable
   - System never fails due to RAG issues
   - Logs when fallback is triggered (âœ… RAG / âš ï¸ Fallback)

4. **âš¡ Parallel LLM Execution**
   - CV and Project evaluated simultaneously
   - 50% faster than sequential (18s vs 36s)
   - Uses `Promise.all()` for clean implementation

5. **ðŸ’¾ Database Connection Pooling**
   - Efficient connections via Supabase PgBouncer
   - Handles concurrent job processing

### Developer Experience Features

6. **ðŸ“š Swagger/OpenAPI Documentation**
   - Interactive API docs at `/api` endpoint
   - Auto-generated from TypeScript decorators
   - Try-it-out functionality for testing

7. **ðŸ” Comprehensive Error Messages**
   - Descriptive error responses
   - HTTP status codes follow REST standards
   - User-friendly messages in production

8. **ðŸ“ Structured Logging**
   - Color-coded console output
   - Context tags (controller, service names)
   - Performance metrics (processing time, tokens used)

9. **ðŸ—„ï¸ Database Migration System**
   - Prisma migrations for version control
   - Safe schema updates
   - Rollback capability

10. **ðŸ“¦ File Storage Management**
    - Organized uploads directory structure
    - Original filename preservation
    - UUID-based file naming (no collisions)

### Reliability Features

11. **â™»ï¸ Job Queue with Retry Logic**
    - 3 automatic retry attempts
    - Exponential backoff (5s, 10s, 20s)
    - Failed job tracking in database

12. **â±ï¸ Timeout Protection**
    - LLM call timeout: 60 seconds
    - Job timeout: 2 minutes
    - Prevents hanging requests

13. **ðŸŽ¯ Rate Limit Detection**
    - Detects HTTP 429 responses
    - Immediately switches to backup provider
    - Logs rate limit events for monitoring

14. **ðŸ“Š Job Status Tracking**
    - 4 states: QUEUED â†’ PROCESSING â†’ COMPLETED/FAILED
    - Timestamps for each state transition
    - Processing duration tracking

15. **ðŸ§¹ Text Sanitization**
    - Removes null bytes (PostgreSQL incompatible)
    - Handles special characters in PDFs
    - Prevents database insertion errors

### Performance Optimizations

16. **ðŸ’° Token Optimization**
    - Truncates RAG context to 2000 chars
    - Prevents exceeding context windows
    - Reduces API costs

17. **ðŸš€ Efficient PDF Parsing**
    - Parses once, stores in database
    - Reusable for multiple evaluations
    - Extracted text cached in `uploaded_files` table

18. **ðŸ”„ Connection Reuse**
    - HTTP keep-alive for LLM APIs
    - Database connection pooling
    - Redis connection reuse

### Production-Ready Features

19. **ðŸ¥ Health Check Endpoint**
    - `/health` for uptime monitoring
    - Checks database connectivity
    - Public endpoint (no auth required)

20. **ðŸŒ CORS Configuration**
    - Configurable allowed origins
    - Ready for frontend integration
    - Production-safe defaults

---
